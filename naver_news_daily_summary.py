import requests
from datetime import datetime, timedelta
import os
from collections import defaultdict
import re
import json
from difflib import SequenceMatcher
import pytz
import pandas as pd
from pathlib import Path
from config import KEYWORDS, DATA_DIR, REPORTS_DIR

# í™˜ê²½ ë³€ìˆ˜
TELEGRAM_BOT_TOKEN = os.environ.get('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID_NEWS')
NAVER_CLIENT_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.environ.get('NAVER_CLIENT_SECRET')

# ì¼ì¼ ìš”ì•½ ì „ìš© ì„¤ì •
DAILY_SUMMARY_COUNT = 50

# ìœ ì‚¬ë„ ì„ê³„ê°’
SIMILARITY_THRESHOLD = float(os.environ.get('SIMILARITY_THRESHOLD', '0.60'))

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

def get_kst_now():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ ë°˜í™˜"""
    return datetime.now(KST)

def get_yesterday_range():
    """ì „ë‚  00:00:00 ~ 23:59:59 ë°˜í™˜"""
    now = get_kst_now()
    yesterday = now - timedelta(days=1)
    
    start = yesterday.replace(hour=0, minute=0, second=0, microsecond=0)
    end = yesterday.replace(hour=23, minute=59, second=59, microsecond=999999)
    
    return start, end

def parse_pub_date(pub_date_str):
    """ë„¤ì´ë²„ API pubDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜"""
    try:
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(pub_date_str)
        return dt.astimezone(KST)
    except:
        return None

def is_within_date_range(pub_date_str, start_dt, end_dt):
    """ë‰´ìŠ¤ê°€ íŠ¹ì • ë‚ ì§œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    pub_dt = parse_pub_date(pub_date_str)
    if not pub_dt:
        return False
    
    return start_dt <= pub_dt <= end_dt

def clean_title(title):
    """ì œëª©ì—ì„œ HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    title = title.replace('<b>', '').replace('</b>', '')
    title = title.replace('&quot;', '"').replace('&amp;', '&')
    title = title.replace('&lt;', '<').replace('&gt;', '>')
    return title.strip()

def normalize_title(title):
    """ì œëª© ì •ê·œí™” (ì¤‘ë³µ ë¹„êµìš©)"""
    title = clean_title(title)
    title = re.sub(r'\s+', ' ', title)
    return title.lower().strip()

def calculate_similarity(title1, title2):
    """ë‘ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0~1.0)"""
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    return SequenceMatcher(None, norm1, norm2).ratio()

def group_similar_news(news_list):
    """ìœ ì‚¬í•œ ì œëª©ì˜ ë‰´ìŠ¤ë¥¼ ê·¸ë£¹í™”"""
    if not news_list:
        return []
    
    groups = []
    used = set()
    
    for i, news in enumerate(news_list):
        if i in used:
            continue
        
        group = [news]
        used.add(i)
        
        for j, other_news in enumerate(news_list):
            if j in used:
                continue
            
            similarity = calculate_similarity(news['title'], other_news['title'])
            
            if similarity >= SIMILARITY_THRESHOLD:
                group.append(other_news)
                used.add(j)
        
        groups.append(group)
    
    return groups

def select_representative_title(group):
    """ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ì œëª© ì„ íƒ (ê°€ì¥ ì •ë³´ê°€ í’ë¶€í•œ ì œëª©)"""
    return max(group, key=lambda x: len(clean_title(x['title'])))

def keyword_exists_in_news(news, keyword):
    """ë‰´ìŠ¤ì— í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    title = clean_title(news.get('title', ''))
    description = clean_title(news.get('description', ''))
    
    keyword_lower = keyword.lower()
    title_lower = title.lower()
    description_lower = description.lower()
    
    return keyword_lower in title_lower or keyword_lower in description_lower

def search_naver_news(keyword, start_dt, end_dt):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API + ë‚ ì§œ ë²”ìœ„ í•„í„°ë§"""
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {
        "query": keyword,
        "display": DAILY_SUMMARY_COUNT,
        "sort": "date"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            all_items = response.json()['items']
            
            keyword_filtered = [
                item for item in all_items 
                if keyword_exists_in_news(item, keyword)
            ]
            
            date_filtered = [
                item for item in keyword_filtered
                if is_within_date_range(item.get('pubDate', ''), start_dt, end_dt)
            ]
            
            print(f"  {keyword}: {len(all_items)}ê°œ ìˆ˜ì§‘ â†’ í‚¤ì›Œë“œ {len(keyword_filtered)}ê°œ â†’ ì „ì¼ {len(date_filtered)}ê°œ")
            
            return date_filtered
        else:
            print(f"Error {response.status_code}: {keyword}")
            return []
    except Exception as e:
        print(f"Exception for {keyword}: {e}")
        return []

def remove_duplicates(all_news_by_keyword):
    """ì¤‘ë³µ ì œê±° - í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ ìš°ì„ ìˆœìœ„ ì ìš©"""
    seen_links = set()
    seen_titles = set()
    deduplicated = defaultdict(list)
    
    for keyword in KEYWORDS:
        if keyword not in all_news_by_keyword:
            continue
            
        for news in all_news_by_keyword[keyword]:
            link = news['link']
            normalized_title = normalize_title(news['title'])
            
            if link in seen_links or normalized_title in seen_titles:
                continue
            
            seen_links.add(link)
            seen_titles.add(normalized_title)
            deduplicated[keyword].append(news)
    
    return deduplicated

def save_data(grouped_news_by_keyword, stats, yesterday_date):
    """ë°ì´í„° ì €ì¥ (JSON, Excel, Markdown)"""
    now = get_kst_now()
    timestamp = yesterday_date.replace("-", "")
    date_str = now.strftime("%Y-%m-%d %H:%M KST")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(DATA_DIR).mkdir(exist_ok=True)
    Path(REPORTS_DIR).mkdir(exist_ok=True)
    
    # JSON ì €ì¥
    json_path = f"{DATA_DIR}/mvno_daily_{timestamp}.json"
    json_data = {
        "report_date": yesterday_date,
        "generated_at": date_str,
        "similarity_threshold": SIMILARITY_THRESHOLD,
        "statistics": stats,
        "news_by_keyword": grouped_news_by_keyword
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ JSON ì €ì¥: {json_path}")
    
    # Excel ì €ì¥
    excel_path = f"{REPORTS_DIR}/mvno_daily_{timestamp}.xlsx"
    excel_data = []
    
    for keyword in KEYWORDS:
        groups = grouped_news_by_keyword.get(keyword, [])
        for group in groups:
            representative = select_representative_title(group)
            excel_data.append({
                "í‚¤ì›Œë“œ": keyword,
                "ì œëª©": clean_title(representative['title']),
                "ë§í¬": representative['link'],
                "ë°œí–‰ì¼": representative['pubDate'],
                "ìœ ì‚¬ê¸°ì‚¬ìˆ˜": len(group) - 1,
                "ê·¸ë£¹í¬ê¸°": len(group)
            })
    
    if excel_data:
        df = pd.DataFrame(excel_data)
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"âœ“ Excel ì €ì¥: {excel_path}")
    
    # Markdown ì €ì¥
    md_path = f"{REPORTS_DIR}/mvno_daily_{timestamp}.md"
    md_content = f"# MVNO ì¼ì¼ ë‰´ìŠ¤ ìš”ì•½\n\n"
    md_content += f"**ë³´ê³  ë‚ ì§œ**: {yesterday_date} (ì „ì¼)\n"
    md_content += f"**ìƒì„± ì‹œê°„**: {date_str}\n"
    md_content += f"**ì´ ë‰´ìŠ¤**: {stats['total_news']}ê°œ\n\n"
    md_content += "---\n\n"
    
    for keyword in KEYWORDS:
        groups = grouped_news_by_keyword.get(keyword, [])
        if groups:
            total_in_keyword = sum(len(group) for group in groups)
            md_content += f"## ğŸ” {keyword} ({total_in_keyword}ê°œ)\n\n"
            
            for idx, group in enumerate(groups, 1):
                representative = select_representative_title(group)
                title = clean_title(representative['title'])
                link = representative['link']
                pub_date = representative['pubDate']
                similar_count = len(group) - 1
                
                md_content += f"### {idx}. {title}\n"
                if similar_count > 0:
                    md_content += f"**ìœ ì‚¬ ê¸°ì‚¬**: {similar_count}ê±´\n"
                md_content += f"**ë§í¬**: {link}\n"
                md_content += f"**ë°œí–‰ì¼**: {pub_date}\n\n"
                
                if similar_count > 0:
                    md_content += "**ìœ ì‚¬ ê¸°ì‚¬ ëª©ë¡**:\n"
                    for similar_news in group[1:]:
                        similar_title = clean_title(similar_news['title'])
                        md_content += f"- {similar_title}\n"
                        md_content += f"  - {similar_news['link']}\n"
                    md_content += "\n"
    
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ“ Markdown ì €ì¥: {md_path}")
    
    return json_path, excel_path, md_path

def send_telegram_summary(stats, yesterday_date, file_paths):
    """í…”ë ˆê·¸ë¨ ìš”ì•½ ì „ì†¡"""
    now = get_kst_now()
    today = now.strftime("%Y-%m-%d %H:%M KST")
    
    message = f"ğŸ“Š <b>MVNO ì¼ì¼ ë‰´ìŠ¤ ìš”ì•½</b>\n\n"
    message += f"ğŸ“… ë³´ê³  ë‚ ì§œ: {yesterday_date} (ì „ì¼)\n"
    message += f"ğŸ• ìƒì„± ì‹œê°„: {today}\n"
    message += f"ğŸ“° ì´ ê¸°ì‚¬: {stats['total_news']}ê°œ\n\n"
    
    if stats['total_news'] > 0:
        message += f"ğŸ“ˆ <b>í‚¤ì›Œë“œë³„ í†µê³„</b>\n"
        for keyword, count in stats['by_keyword'].items():
            if count > 0:
                message += f"  â€¢ {keyword}: {count}ê°œ\n"
        message += "\n"
    
    message += f"ğŸ’¾ <b>ì €ì¥ íŒŒì¼</b>\n"
    message += f"  â€¢ JSON: {file_paths['json']}\n"
    message += f"  â€¢ Excel: {file_paths['excel']}\n"
    message += f"  â€¢ Markdown: {file_paths['markdown']}\n"
    
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML"
    }
    
    try:
        response = requests.post(url, data=data)
        return response.json()
    except Exception as e:
        print(f"Telegram error: {e}")
        return None

def main():
    now = get_kst_now()
    start_dt, end_dt = get_yesterday_range()
    
    today = now.strftime("%Y-%m-%d %H:%M KST")
    yesterday_date = start_dt.strftime("%Y-%m-%d")
    
    print(f"Starting daily news summary at {today}...")
    print(f"Collection period: {yesterday_date} 00:00 ~ 23:59")
    print(f"Similarity threshold: {SIMILARITY_THRESHOLD}")
    
    # 1ë‹¨ê³„: ëª¨ë“  í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_news_by_keyword = {}
    
    for keyword in KEYWORDS:
        print(f"Searching: {keyword}")
        news_list = search_naver_news(keyword, start_dt, end_dt)
        all_news_by_keyword[keyword] = news_list
    
    # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±°
    print("\nRemoving duplicates...")
    deduplicated_news = remove_duplicates(all_news_by_keyword)
    
    # 3ë‹¨ê³„: ìœ ì‚¬ ì œëª© ê·¸ë£¹í™”
    print("\nGrouping similar news...")
    grouped_news_by_keyword = {}
    stats = {
        'total_news': 0,
        'by_keyword': {}
    }
    
    for keyword, news_list in deduplicated_news.items():
        groups = group_similar_news(news_list)
        grouped_news_by_keyword[keyword] = groups
        
        total_articles = len(news_list)
        num_groups = len(groups)
        similar_count = sum(len(g) - 1 for g in groups if len(g) > 1)
        
        stats['total_news'] += total_articles
        stats['by_keyword'][keyword] = total_articles
        
        print(f"  {keyword}: {total_articles}ê°œ â†’ {num_groups}ê°œ ê·¸ë£¹ (ìœ ì‚¬ {similar_count}ê±´)")
    
    print(f"\nTotal articles: {stats['total_news']}")
    
    # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if stats['total_news'] == 0:
        print("No articles found for yesterday. Exiting...")
        return
    
    # 4ë‹¨ê³„: ë°ì´í„° ì €ì¥
    print("\nSaving data...")
    json_path, excel_path, md_path = save_data(grouped_news_by_keyword, stats, yesterday_date)
    
    # 5ë‹¨ê³„: í…”ë ˆê·¸ë¨ ìš”ì•½ ì „ì†¡
    print("\nSending Telegram summary...")
    file_paths = {
        'json': json_path,
        'excel': excel_path,
        'markdown': md_path
    }
    send_telegram_summary(stats, yesterday_date, file_paths)
    
    print("\nâœ… Completed!")
    print(f"ğŸ“Š Total: {stats['total_news']} articles")
    print(f"ğŸ’¾ Files saved in: {DATA_DIR}/, {REPORTS_DIR}/")

if __name__ == "__main__":
    main()
