import requests
from datetime import datetime, timedelta
import os
from collections import defaultdict
import re
import json
from difflib import SequenceMatcher
import pytz
import pandas as pd
from pathlib import Path
from config import KEYWORDS, NEWS_COUNT, DATA_DIR, REPORTS_DIR

# í™˜ê²½ ë³€ìˆ˜
TELEGRAM_BOT_TOKEN = os.environ.get('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.environ.get('TELEGRAM_CHAT_ID_NEWS')
NAVER_CLIENT_ID = os.environ.get('NAVER_CLIENT_ID')
NAVER_CLIENT_SECRET = os.environ.get('NAVER_CLIENT_SECRET')

# ê²€ìƒ‰ ê¸°ê°„ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜, ê¸°ë³¸ê°’ 3ì‹œê°„)
SEARCH_HOURS = int(os.environ.get('SEARCH_HOURS', '3'))

# ìœ ì‚¬ë„ ì„ê³„ê°’
SIMILARITY_THRESHOLD = float(os.environ.get('SIMILARITY_THRESHOLD', '0.60'))

# í•œêµ­ ì‹œê°„ëŒ€ ì„¤ì •
KST = pytz.timezone('Asia/Seoul')

def get_kst_now():
    """í˜„ì¬ í•œêµ­ ì‹œê°„ ë°˜í™˜"""
    return datetime.now(KST)

def parse_pub_date(pub_date_str):
    """ë„¤ì´ë²„ API pubDateë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
    ì˜ˆ: 'Sun, 16 Nov 2025 10:00:00 +0900'
    """
    try:
        from email.utils import parsedate_to_datetime
        dt = parsedate_to_datetime(pub_date_str)
        return dt.astimezone(KST)
    except:
        return None

def is_within_search_period(pub_date_str, hours):
    """ë‰´ìŠ¤ê°€ ê²€ìƒ‰ ê¸°ê°„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸"""
    pub_dt = parse_pub_date(pub_date_str)
    if not pub_dt:
        return True
    
    now = get_kst_now()
    time_diff = now - pub_dt
    
    return time_diff <= timedelta(hours=hours)

def clean_title(title):
    """ì œëª©ì—ì„œ HTML íƒœê·¸ ë° íŠ¹ìˆ˜ë¬¸ì ì œê±°"""
    title = title.replace('<b>', '').replace('</b>', '')
    title = title.replace('&quot;', '"').replace('&amp;', '&')
    title = title.replace('&lt;', '<').replace('&gt;', '>')
    return title.strip()

def normalize_title(title):
    """ì œëª© ì •ê·œí™” (ì¤‘ë³µ ë¹„êµìš©)"""
    title = clean_title(title)
    title = re.sub(r'\s+', ' ', title)
    return title.lower().strip()

def calculate_similarity(title1, title2):
    """ë‘ ì œëª© ê°„ì˜ ìœ ì‚¬ë„ ê³„ì‚° (0.0~1.0)"""
    norm1 = normalize_title(title1)
    norm2 = normalize_title(title2)
    return SequenceMatcher(None, norm1, norm2).ratio()

def group_similar_news(news_list):
    """ìœ ì‚¬í•œ ì œëª©ì˜ ë‰´ìŠ¤ë¥¼ ê·¸ë£¹í™”"""
    if not news_list:
        return []
    
    groups = []
    used = set()
    
    for i, news in enumerate(news_list):
        if i in used:
            continue
        
        group = [news]
        used.add(i)
        
        for j, other_news in enumerate(news_list):
            if j in used:
                continue
            
            similarity = calculate_similarity(news['title'], other_news['title'])
            
            if similarity >= SIMILARITY_THRESHOLD:
                group.append(other_news)
                used.add(j)
        
        groups.append(group)
    
    return groups

def select_representative_title(group):
    """ê·¸ë£¹ì—ì„œ ëŒ€í‘œ ì œëª© ì„ íƒ (ê°€ì¥ ì •ë³´ê°€ í’ë¶€í•œ ì œëª©)"""
    return max(group, key=lambda x: len(clean_title(x['title'])))

def keyword_exists_in_news(news, keyword):
    """ë‰´ìŠ¤ì— í‚¤ì›Œë“œê°€ ì‹¤ì œë¡œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
    title = clean_title(news.get('title', ''))
    description = clean_title(news.get('description', ''))
    
    keyword_lower = keyword.lower()
    title_lower = title.lower()
    description_lower = description.lower()
    
    return keyword_lower in title_lower or keyword_lower in description_lower

def search_naver_news(keyword):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ API + í‚¤ì›Œë“œ í•„í„°ë§ + ê¸°ê°„ í•„í„°ë§"""
    url = "https://openapi.naver.com/v1/search/news.json"
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
    }
    params = {
        "query": keyword,
        "display": NEWS_COUNT * 3,
        "sort": "date"
    }
    
    try:
        response = requests.get(url, headers=headers, params=params)
        if response.status_code == 200:
            all_items = response.json()['items']
            
            keyword_filtered = [
                item for item in all_items 
                if keyword_exists_in_news(item, keyword)
            ]
            
            period_filtered = [
                item for item in keyword_filtered
                if is_within_search_period(item.get('pubDate', ''), SEARCH_HOURS)
            ]
            
            print(f"  {keyword}: {len(all_items)}ê°œ ìˆ˜ì§‘ â†’ í‚¤ì›Œë“œ {len(keyword_filtered)}ê°œ â†’ ê¸°ê°„ {len(period_filtered)}ê°œ (ìµœê·¼ {SEARCH_HOURS}ì‹œê°„)")
            
            return period_filtered[:NEWS_COUNT]
        else:
            print(f"Error {response.status_code}: {keyword}")
            return []
    except Exception as e:
        print(f"Exception for {keyword}: {e}")
        return []

def load_existing_news():
    """ê¸°ì¡´ ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ (ì¤‘ë³µ ë°©ì§€ìš©)"""
    data_path = Path(DATA_DIR)
    existing_links = set()
    
    if data_path.exists():
        for json_file in data_path.glob("mvno_news_*.json"):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for keyword_data in data.get('news_by_keyword', {}).values():
                        for group in keyword_data:
                            for news in group:
                                existing_links.add(news['link'])
            except:
                continue
    
    return existing_links

def remove_duplicates(all_news_by_keyword, existing_links):
    """ì¤‘ë³µ ì œê±° - í‚¤ì›Œë“œ ìˆœì„œëŒ€ë¡œ ìš°ì„ ìˆœìœ„ ì ìš© + ê¸°ì¡´ ë‰´ìŠ¤ ì œì™¸"""
    seen_links = existing_links.copy()
    seen_titles = set()
    deduplicated = defaultdict(list)
    
    for keyword in KEYWORDS:
        if keyword not in all_news_by_keyword:
            continue
            
        for news in all_news_by_keyword[keyword]:
            link = news['link']
            normalized_title = normalize_title(news['title'])
            
            if link in seen_links or normalized_title in seen_titles:
                continue
            
            seen_links.add(link)
            seen_titles.add(normalized_title)
            deduplicated[keyword].append(news)
    
    return deduplicated

def save_data(grouped_news_by_keyword, stats):
    """ë°ì´í„° ì €ì¥ (JSON, Excel, Markdown)"""
    now = get_kst_now()
    timestamp = now.strftime("%Y%m%d_%H%M%S")
    date_str = now.strftime("%Y-%m-%d %H:%M KST")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    Path(DATA_DIR).mkdir(exist_ok=True)
    Path(REPORTS_DIR).mkdir(exist_ok=True)
    
    # JSON ì €ì¥
    json_path = f"{DATA_DIR}/mvno_news_{timestamp}.json"
    json_data = {
        "collection_time": date_str,
        "search_hours": SEARCH_HOURS,
        "similarity_threshold": SIMILARITY_THRESHOLD,
        "statistics": stats,
        "news_by_keyword": grouped_news_by_keyword
    }
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ JSON ì €ì¥: {json_path}")
    
    # Excel ì €ì¥
    excel_path = f"{REPORTS_DIR}/mvno_news_{timestamp}.xlsx"
    excel_data = []
    
    for keyword in KEYWORDS:
        groups = grouped_news_by_keyword.get(keyword, [])
        for group in groups:
            representative = select_representative_title(group)
            excel_data.append({
                "í‚¤ì›Œë“œ": keyword,
                "ì œëª©": clean_title(representative['title']),
                "ë§í¬": representative['link'],
                "ë°œí–‰ì¼": representative['pubDate'],
                "ìœ ì‚¬ê¸°ì‚¬ìˆ˜": len(group) - 1,
                "ê·¸ë£¹í¬ê¸°": len(group)
            })
    
    if excel_data:
        df = pd.DataFrame(excel_data)
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"âœ“ Excel ì €ì¥: {excel_path}")
    
    # Markdown ì €ì¥
    md_path = f"{REPORTS_DIR}/mvno_news_{timestamp}.md"
    md_content = f"# MVNO ë‰´ìŠ¤ ëª¨ìŒ\n\n"
    md_content += f"**ìˆ˜ì§‘ ì‹œê°„**: {date_str}\n"
    md_content += f"**ê²€ìƒ‰ ê¸°ê°„**: ìµœê·¼ {SEARCH_HOURS}ì‹œê°„\n"
    md_content += f"**ì´ ë‰´ìŠ¤**: {stats['total_news']}ê°œ\n\n"
    md_content += "---\n\n"
    
    for keyword in KEYWORDS:
        groups = grouped_news_by_keyword.get(keyword, [])
        if groups:
            total_in_keyword = sum(len(group) for group in groups)
            md_content += f"## ğŸ” {keyword} ({total_in_keyword}ê°œ)\n\n"
            
            for idx, group in enumerate(groups, 1):
                representative = select_representative_title(group)
                title = clean_title(representative['title'])
                link = representative['link']
                pub_date = representative['pubDate']
                similar_count = len(group) - 1
                
                md_content += f"### {idx}. {title}\n"
                if similar_count > 0:
                    md_content += f"**ìœ ì‚¬ ê¸°ì‚¬**: {similar_count}ê±´\n"
                md_content += f"**ë§í¬**: {link}\n"
                md_content += f"**ë°œí–‰ì¼**: {pub_date}\n\n"
                
                if similar_count > 0:
                    md_content += "**ìœ ì‚¬ ê¸°ì‚¬ ëª©ë¡**:\n"
                    for similar_news in group[1:]:
                        similar_title = clean_title(similar_news['title'])
                        md_content += f"- {similar_title}\n"
                        md_content += f"  - {similar_news['link']}\n"
                    md_content += "\n"
    
    with open(md_path, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"âœ“ Markdown ì €ì¥: {md_path}")
    
    return json_path, excel_path, md_path

def send_telegram_summary(stats, file_paths):
    """í…”ë ˆê·¸ë¨ ìš”ì•½ ì „ì†¡ (íŒŒì¼ ê²½ë¡œë§Œ)"""
    now = get_kst_now()
    today = now.strftime("%Y-%m-%d %H:%M KST")
    
    message = f"ğŸ“° <b>MVNO ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ</b>\n\n"
    message += f"ğŸ“… {today}\n"
    message += f"â±ï¸ ìµœê·¼ {SEARCH_HOURS}ì‹œê°„ ë‰´ìŠ¤\n"
    message += f"ğŸ“Š ìƒˆ ë‰´ìŠ¤: {stats['total_news']}ê°œ\n\n"
    
    if stats['total_news'] > 0:
        message += f"ğŸ“ˆ <b>í‚¤ì›Œë“œë³„ í†µê³„</b>\n"
        for keyword, count in stats['by_keyword'].items():
            if count > 0:
                message += f"  â€¢ {keyword}: {count}ê°œ\n"
        message += "\n"
    
    message += f"ğŸ’¾ <b>ì €ì¥ íŒŒì¼</b>\n"
    message += f"  â€¢ JSON: {file_paths['json']}\n"
    message += f"  â€¢ Excel: {file_paths['excel']}\n"
    message += f"  â€¢ Markdown: {file_paths['markdown']}\n"
    
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
    data = {
        "chat_id": TELEGRAM_CHAT_ID,
        "text": message,
        "parse_mode": "HTML"
    }
    
    try:
        response = requests.post(url, data=data)
        return response.json()
    except Exception as e:
        print(f"Telegram error: {e}")
        return None

def main():
    now = get_kst_now()
    today = now.strftime("%Y-%m-%d %H:%M KST")
    
    print(f"Starting MVNO news collection at {today}...")
    print(f"Search period: Last {SEARCH_HOURS} hours")
    print(f"Similarity threshold: {SIMILARITY_THRESHOLD}")
    
    # ê¸°ì¡´ ë‰´ìŠ¤ ë¡œë“œ
    existing_links = load_existing_news()
    print(f"Loaded existing links: {len(existing_links)}")
    
    # 1ë‹¨ê³„: ëª¨ë“  í‚¤ì›Œë“œì˜ ë‰´ìŠ¤ ìˆ˜ì§‘
    all_news_by_keyword = {}
    
    for keyword in KEYWORDS:
        print(f"Searching: {keyword}")
        news_list = search_naver_news(keyword)
        all_news_by_keyword[keyword] = news_list
    
    # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±° (ê¸°ì¡´ ë‰´ìŠ¤ í¬í•¨)
    print("\nRemoving duplicates...")
    deduplicated_news = remove_duplicates(all_news_by_keyword, existing_links)
    
    # 3ë‹¨ê³„: ìœ ì‚¬ ì œëª© ê·¸ë£¹í™”
    print("\nGrouping similar news...")
    grouped_news_by_keyword = {}
    stats = {
        'total_news': 0,
        'by_keyword': {}
    }
    
    for keyword, news_list in deduplicated_news.items():
        groups = group_similar_news(news_list)
        grouped_news_by_keyword[keyword] = groups
        
        total_articles = len(news_list)
        num_groups = len(groups)
        similar_count = sum(len(g) - 1 for g in groups if len(g) > 1)
        
        stats['total_news'] += total_articles
        stats['by_keyword'][keyword] = total_articles
        
        print(f"  {keyword}: {total_articles}ê°œ â†’ {num_groups}ê°œ ê·¸ë£¹ (ìœ ì‚¬ {similar_count}ê±´)")
    
    print(f"\nTotal new articles: {stats['total_news']}")
    
    # ìƒˆ ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if stats['total_news'] == 0:
        print("No new articles. Exiting...")
        return
    
    # 4ë‹¨ê³„: ë°ì´í„° ì €ì¥
    print("\nSaving data...")
    json_path, excel_path, md_path = save_data(grouped_news_by_keyword, stats)
    
    # 5ë‹¨ê³„: í…”ë ˆê·¸ë¨ ìš”ì•½ ì „ì†¡
    print("\nSending Telegram summary...")
    file_paths = {
        'json': json_path,
        'excel': excel_path,
        'markdown': md_path
    }
    send_telegram_summary(stats, file_paths)
    
    print("\nâœ… Completed!")
    print(f"ğŸ“Š Total: {stats['total_news']} new articles")
    print(f"ğŸ’¾ Files saved in: {DATA_DIR}/, {REPORTS_DIR}/")

if __name__ == "__main__":
    main()
